{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "old_path = 'D:/Audio Analysis Notebook/data'\n",
    "new_path = 'D:/Audio Analysis Notebook/new data'\n",
    "\n",
    "files = []\n",
    "data_folders = os.listdir(old_path)\n",
    "\n",
    "#Separating audio clips on the basis of 8 emotions and storing them together accordingly\n",
    "#Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
    "for i in data_folders:\n",
    "    temp_join = join(old_path,i)\n",
    "    for j in listdir(temp_join):\n",
    "        if isfile(join(temp_join,j)):\n",
    "            files.append(j)\n",
    "            temp = j.split('.')[0].split('-')\n",
    "            subprocess.call(\"mv %s %s\" % (join(temp_join,j),join(new_path,temp[2])),shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a012f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import webrtcvad\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c22b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wave(path):\n",
    "    \"\"\"Reads a .wav file.\n",
    "    Takes the path, and returns (PCM audio data, sample rate).\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate\n",
    "\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"Writes a .wav file.\n",
    "    Takes path, PCM audio data, and sample rate.\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "\n",
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms,\n",
    "                  padding_duration_ms, vad, frames):\n",
    "    \"\"\"Filters out non-voiced audio frames.\n",
    "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "    the voiced audio.\n",
    "    Uses a padded, sliding window algorithm over the audio frames.\n",
    "    When more than 90% of the frames in the window are voiced (as\n",
    "    reported by the VAD), the collector triggers and begins yielding\n",
    "    audio frames. Then the collector waits until 90% of the frames in\n",
    "    the window are unvoiced to detrigger.\n",
    "    The window is padded at the front and back to provide a small\n",
    "    amount of silence or the beginnings/endings of speech around the\n",
    "    voiced frames.\n",
    "    Arguments:\n",
    "    sample_rate - The audio sample rate, in Hz.\n",
    "    frame_duration_ms - The frame duration in milliseconds.\n",
    "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "    vad - An instance of webrtcvad.Vad.\n",
    "    frames - a source of audio frames (sequence or generator).\n",
    "    Returns: A generator that yields PCM audio data.\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        sys.stdout.write('1' if is_speech else '0')\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "                triggered = False\n",
    "                yield b''.join([f.bytes for f in voiced_frames])\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if triggered:\n",
    "        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "    sys.stdout.write('\\n')\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "    if voiced_frames:\n",
    "        yield b''.join([f.bytes for f in voiced_frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b794697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000000000000000000000000000000000000000000000000000000000011111111001111111111+(2.549999999999998)111111111111001111111111111111110000000000-(4.10999999999999) Writing chunk-00.wav\n",
      "000000000001111111111+(4.439999999999992)1111111000000000111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111101111110000000111111111111111111111111111111111111111111111111111111111111111111111110000000000-(15.56999999999986) Writing chunk-01.wav\n",
      "000000000111110000000000000001111111111+(16.439999999999866)11111111111111111111111111110000000000-(17.87999999999992) Writing chunk-02.wav\n",
      "0000000000000000000000000000000000000000000000000000000000111111100111111110001110001111111111+(20.400000000000016)111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(24.840000000000185) Writing chunk-03.wav\n",
      "00000001111111111+(25.050000000000193)1111111111111111111111110000000000-(26.370000000000243) Writing chunk-04.wav\n",
      "000000000000000000000000000000000000000000000001111111111+(27.780000000000296)111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111001110000000000-(34.470000000000546) Writing chunk-05.wav\n",
      "0000000000000000000000001111111111+(35.19000000000057)1110000000000-(35.8800000000006) Writing chunk-06.wav\n",
      "00000000000000000000000000000000000000000011100000000000000000000000000000000000000000000111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001111111111+(44.64000000000093)1111111111110000000000-(45.60000000000097) Writing chunk-07.wav\n",
      "00001111111111+(45.72000000000097)1111111111111111111111111111111111111111111111111110000000000-(47.85000000000105) Writing chunk-08.wav\n",
      "0001111111111+(47.940000000001056)111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000011111000000111111111111110000011111111111111111111000000000111111110000000000-(56.880000000001395) Writing chunk-09.wav\n",
      "000000000000000000000000000000000000000000000000000000001111111111+(58.56000000000146)11111111111111111111111000011111111111111111111111111111111001110000000000-(61.080000000001554) Writing chunk-10.wav\n",
      "00000001111111111+(61.29000000000156)1111111111111111111111111111111111111111111111111111110000000000-(63.51000000000165) Writing chunk-11.wav\n",
      "0000000000000001111111111+(63.96000000000166)11111111111000111111111111111111111111111111111111111110000000000-(66.21000000000174) Writing chunk-12.wav\n",
      "01111111111+(66.24000000000174)11111111111111111111111111111111110000000000-(67.8600000000018) Writing chunk-13.wav\n",
      "1111111111+(67.8600000000018)111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111101111111111111111111111111111111111111111111111111111111111111111111111111111111111111011100000000011111111111111111111111111111111111111100001110001111000111100011100000111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(80.9400000000023) Writing chunk-14.wav\n",
      "000000001111111111+(81.18000000000231)1111111111111111111111111111111100111111111111110011100111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(85.92000000000249) Writing chunk-15.wav\n",
      "000000000001111111111+(86.2500000000025)11111111111111111111111111111111110000000000-(87.87000000000256) Writing chunk-16.wav\n",
      "0001111111111+(87.96000000000257)11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(98.31000000000296) Writing chunk-17.wav\n",
      "00000000000000000000000001111111111+(99.06000000000299)1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111011111111111111111111111111111111111111110000000000-(104.34000000000319) Writing chunk-18.wav\n",
      "00001111111111+(104.46000000000319)1111111111111111111111111111111111111111111111111111110000000000-(106.68000000000328) Writing chunk-19.wav\n",
      "0000001111111111+(106.86000000000328)1111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(109.71000000000339) Writing chunk-20.wav\n",
      "0111000000001111111111+(110.0700000000034)1111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000001111111111111111111111111111111111111111111111111111111111111111111111111111100001111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111110000000000-(118.68000000000373) Writing chunk-21.wav\n",
      "00000000000000000001111111111+(119.25000000000375)111111111111110000000000-(120.27000000000379) Writing chunk-22.wav\n",
      "000000001111111100000001111111111+(120.96000000000382)1111111111111110111111111111111000000011111000001111-(122.82000000000389)\n",
      " Writing chunk-23.wav\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "audio, sample_rate = read_wave('D:/Audio Analysis Notebook/Recordings/20211201-083202_8008512300_1003_TOLLFREE-all.wav')\n",
    "vad = webrtcvad.Vad(2)\n",
    "frames = frame_generator(30, audio, sample_rate)\n",
    "frames = list(frames)\n",
    "segments = vad_collector(sample_rate, 30, 300, vad, frames)\n",
    "c = 0\n",
    "for i, segment in enumerate(segments):\n",
    "    path = 'chunk-%002d.wav' % (i,)\n",
    "    print(' Writing %s' % (path,))\n",
    "    write_wave(path, segment, sample_rate)\n",
    "    c +=1\n",
    "#count of chunks\n",
    "# c = 14\n",
    "\n",
    "sampling_rate = 8000\n",
    "n_mfcc = 13\n",
    "n_fft = 0.032\n",
    "hop_length = 0.010\n",
    "\n",
    "components = 16\n",
    "\n",
    "cov_type = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ccf16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708446,)\n",
      "-29.559303948184127\n",
      "[[-5.56621278e+02  1.05131404e+02  2.27655504e+01  1.46333235e+01\n",
      "   1.77650447e+01  6.61958577e+00  8.55127364e+00  6.34394177e+00\n",
      "  -1.60606648e-01  3.14688676e+00  1.38520897e-01 -3.94708474e+00\n",
      "   1.82335051e+00 -9.69751687e-01 -6.18997197e-01  4.11447889e-01\n",
      "   4.26958872e-01  6.60999847e-02  2.06966301e-02  6.64566335e-02\n",
      "   2.61113608e-02 -4.20451190e-02 -4.03467753e-02  5.24076874e-02\n",
      "   5.51030654e-02 -1.32705005e-02  2.78081783e-01  1.92665218e-01\n",
      "  -5.90249949e-02 -5.86755319e-02 -2.61258075e-02 -5.24293144e-02\n",
      "  -4.71707214e-02 -4.25137752e-02 -3.32234967e-02 -8.43542689e-03\n",
      "  -1.38019126e-02 -2.94477548e-03  3.31042194e-02]\n",
      " [-3.68091082e+02  1.90953295e+02 -6.69177489e+01 -1.59496686e-01\n",
      "   2.34129439e+01 -3.97317458e+01 -8.25411535e+00 -1.19958596e+00\n",
      "  -2.18714096e+01  8.28293517e+00  1.73828161e+00 -1.77960697e+01\n",
      "   3.80878338e+00 -1.10035176e+00 -5.49074759e-01  7.07995235e-01\n",
      "   4.29907411e-01 -5.71723241e-02  2.90728380e-01  3.39852244e-01\n",
      "  -9.34364163e-02 -9.30524751e-02 -9.57417814e-03 -1.67853245e-01\n",
      "  -3.87106854e-02  5.77024566e-02 -5.76148652e-01 -2.66531223e-01\n",
      "   2.43890917e-01  6.61350714e-02  6.65653198e-02  9.10102196e-02\n",
      "  -5.05826244e-02  6.77576835e-02  5.83384275e-02 -1.32925783e-01\n",
      "  -9.22772084e-03  8.13114194e-02 -1.04286515e-02]\n",
      " [-4.76970138e+02  1.22074719e+02 -4.37398395e+01  3.39420765e+00\n",
      "   2.21132154e+01 -1.46793635e+01  8.47620975e+00  9.91982987e+00\n",
      "  -7.85333151e+00  1.04217263e+01  5.43677019e+00 -6.98387634e+00\n",
      "   7.59566831e+00 -3.78578207e+00 -2.07884662e+00  1.21930141e+00\n",
      "   1.34484731e-01 -3.58244967e-02  9.18681344e-01  3.69500426e-01\n",
      "   3.46460538e-01  7.47939778e-01  1.29206972e-01 -2.85024874e-02\n",
      "   2.29340211e-01 -1.76345485e-02 -2.28018374e-01 -4.59080298e-02\n",
      "   1.81357242e-01  5.24869924e-02  6.24659748e-02  5.55990490e-02\n",
      "  -1.02009259e-01 -5.20805294e-02  2.10926941e-02 -6.02580848e-02\n",
      "  -3.40584888e-02 -5.68587262e-03 -8.56942054e-02]\n",
      " [-3.46179990e+02  2.29491110e+02 -6.52058215e+01 -4.98214249e+01\n",
      "   1.00736839e+00 -3.69840574e+01 -1.42026369e+01  6.66822879e+00\n",
      "  -2.11834643e+01 -3.15240001e+00  1.91370008e+01 -3.62942706e+00\n",
      "  -7.68414740e+00  5.97333636e-01 -6.05216769e-01 -6.29289572e-01\n",
      "   1.22684798e+00 -1.93626438e-01 -1.15158431e+00  1.02406234e+00\n",
      "   9.46968285e-01 -5.56025512e-01  2.03988990e-01  4.40780013e-01\n",
      "  -3.60013124e-01  1.33455357e-01 -2.01126506e+00 -1.16242850e+00\n",
      "   8.93333154e-01  7.56698977e-01  4.47055729e-01  5.29908534e-01\n",
      "  -9.29861088e-02 -3.35938885e-01  1.07146206e-01 -1.26823620e-01\n",
      "  -4.55185985e-01 -5.29966721e-02  9.49278076e-02]\n",
      " [-5.05146696e+02  1.31463283e+02  7.66988275e+00  2.50229001e+01\n",
      "   2.86935608e+01 -1.08727657e+01 -5.64786601e+00  1.06751445e+00\n",
      "  -9.92408457e+00 -2.83127262e+00 -1.85122297e+00 -6.34571305e+00\n",
      "   2.02182519e+00 -5.78585056e-01 -4.96790443e-01  1.60533718e-02\n",
      "   2.85906693e-01  3.36666090e-01  1.26133142e-01 -5.23182869e-02\n",
      "   5.30953258e-02  1.63250872e-02 -1.64221268e-01 -4.89842524e-02\n",
      "   9.17192978e-02  2.45264629e-02  3.99615313e-01  2.55117450e-01\n",
      "  -2.41062959e-01 -3.20621353e-01 -1.10610881e-01 -1.66968973e-02\n",
      "   1.38766031e-02  7.98197687e-02  6.33609013e-02  4.18430506e-02\n",
      "   7.49825827e-02  3.77583188e-02  6.23799177e-03]\n",
      " [-4.45147790e+02  1.57552505e+02 -3.03276396e+01  7.62061245e+00\n",
      "   2.89483738e+01 -2.33930780e+01 -1.04414255e+01  5.07739157e+00\n",
      "  -1.21429621e+01 -6.34920434e-01  4.11929552e+00 -7.99299557e+00\n",
      "   8.07862742e-01  4.60900767e+00  2.65741682e+00 -2.07180279e+00\n",
      "  -1.23385810e+00  1.61746664e-01 -9.39736116e-01 -1.02075414e+00\n",
      "  -1.72963307e-01 -3.68480888e-01 -2.37913355e-01  1.65204552e-01\n",
      "  -3.22530890e-01 -2.82649441e-01  1.24135479e+00  4.03112582e-01\n",
      "  -6.82502534e-01  1.11918793e-01  5.70223419e-02 -4.65728661e-01\n",
      "  -3.66823040e-02  6.36511341e-02 -1.94858758e-01 -4.34806810e-02\n",
      "  -3.22862169e-02 -7.95615469e-02 -3.64242121e-03]\n",
      " [-6.74556878e+02  8.18391555e-01  5.05709199e-01  2.37645019e-01\n",
      "   1.10099740e-01  6.31837625e-02  1.57123616e-02 -4.05265072e-02\n",
      "  -7.51232665e-02 -8.21994615e-02 -8.33623538e-02 -9.42773244e-02\n",
      "  -1.08779280e-01  1.68296222e-03 -8.12348139e-04 -7.06638348e-03\n",
      "  -1.03749364e-02 -8.77215352e-03 -5.02414756e-03 -2.03632936e-03\n",
      "  -5.20922520e-04  3.65670208e-04  1.39243215e-03  1.64746289e-03\n",
      "   1.10230641e-03  2.84682920e-03  2.03874875e-02  2.09805172e-02\n",
      "   6.43848303e-03 -1.94718339e-03 -3.53479300e-03 -4.16223260e-03\n",
      "  -6.66161092e-03 -9.22892537e-03 -7.73678624e-03 -2.03919561e-03\n",
      "   2.18229777e-03  1.67662301e-03  9.75905949e-05]\n",
      " [-4.61524295e+02  1.79635861e+02  1.39682253e+01 -9.97078292e+00\n",
      "  -2.80569029e+00 -1.37879226e+01 -3.23169161e+00 -6.82248255e+00\n",
      "  -1.73002896e+01 -5.78552205e+00 -4.25422791e+00 -7.30316972e+00\n",
      "   9.05680743e-01 -1.58814524e+00 -7.54935777e-01  7.62146733e-01\n",
      "   2.28637593e-01 -3.54944796e-02  2.25174218e-01  1.44295630e-02\n",
      "   1.36512911e-01  3.48252629e-01  3.11781990e-02 -5.24960869e-02\n",
      "   8.09891911e-02  1.97646250e-02  2.96340523e-01  1.00942276e-01\n",
      "  -3.33384782e-01 -2.61047299e-01 -2.02235476e-02 -8.93063972e-02\n",
      "  -1.37549210e-01  7.92148632e-02  1.51315898e-01  1.92765005e-02\n",
      "   4.13659834e-02  1.28316469e-01  8.05686401e-02]\n",
      " [-3.97547059e+02  2.17662424e+02 -1.54237053e+01 -3.17278216e+01\n",
      "  -2.32021044e+00 -2.60701036e+01 -2.00303602e+01 -7.08782544e+00\n",
      "  -1.55693544e+01 -1.26215717e+01 -4.55133187e+00  9.29924595e-01\n",
      "   2.48856103e+00 -1.05009752e+00 -9.74856828e-01 -4.49364392e-02\n",
      "   4.30525253e-01  5.02281282e-01  4.98932045e-01  2.74780470e-01\n",
      "   2.54042026e-02  2.91967936e-02 -1.00196829e-01 -2.56072068e-01\n",
      "  -3.33683146e-02  1.62714953e-02 -8.04524749e-01 -6.39238572e-01\n",
      "   2.62745369e-01  5.41229660e-01  1.92748221e-01  9.24622640e-02\n",
      "   2.66192432e-01  1.09636912e-01 -6.67371145e-02  9.94123366e-02\n",
      "   5.68396182e-02 -1.93092600e-01 -5.79684395e-02]\n",
      " [-5.89331140e+02  8.14551773e+01  2.85146481e+01  2.12023522e+01\n",
      "   1.91034474e+01  9.87458859e+00  1.24939719e+01  1.14454823e+01\n",
      "   5.00153362e+00  5.77416519e+00  4.10098870e+00  1.20916732e+00\n",
      "   4.18813863e+00 -1.12301096e-01 -3.56347675e-02  7.66626967e-02\n",
      "   1.04932135e-02 -1.13543875e-02  2.72713724e-02  1.17145694e-02\n",
      "   5.93457716e-03  1.77916108e-02  1.11445629e-02  1.82946394e-02\n",
      "   7.37540409e-03 -2.18283024e-02  5.87998070e-02  2.91121067e-02\n",
      "  -2.53251100e-02 -3.17690372e-03  5.76159120e-03 -1.36851320e-02\n",
      "  -1.08818497e-02 -5.45614168e-03 -6.45732949e-03 -9.81721136e-04\n",
      "  -4.19018925e-03 -8.74042149e-03 -5.74475282e-03]\n",
      " [-4.50272252e+02  1.43661635e+02 -1.90184301e+01  5.00082697e+01\n",
      "   3.25227849e+01 -4.62994206e+01 -3.27808089e+00  1.06862604e+01\n",
      "  -2.68752862e+01 -2.97001689e+00  6.93451765e+00 -1.30369343e+01\n",
      "  -1.11989087e+00 -4.83039200e+00 -2.03953512e+00  2.41333100e+00\n",
      "  -9.27646046e-02 -6.73700407e-01  1.40822710e+00  6.20066555e-01\n",
      "  -2.65233659e-01  3.71956447e-01  1.02629176e-01 -2.05157928e-02\n",
      "   4.34195343e-01  1.31679215e-01  6.13040536e-01  1.78921769e-01\n",
      "  -5.03129619e-01 -2.32500088e-01 -1.08016873e-01 -1.55505072e-01\n",
      "   1.05149597e-01  8.66793802e-02 -9.48451246e-02  7.05678098e-02\n",
      "   1.26036034e-01  1.08791495e-02  1.15092612e-01]\n",
      " [-4.58608084e+02  1.61585781e+02 -1.50248205e+01  2.87354102e+00\n",
      "   2.72907909e+01 -2.19195492e+01 -1.80939243e+01  6.67231888e+00\n",
      "  -1.05891398e+01 -9.28022408e+00  6.33593591e+00 -3.96599434e+00\n",
      "  -5.09560274e+00  3.08770121e+00  1.84526486e+00 -1.20278994e+00\n",
      "  -7.95441865e-01 -1.89264484e-01 -8.00610675e-01 -5.23159936e-01\n",
      "   3.53795991e-03 -2.45504768e-01 -1.34694874e-01  1.94586525e-01\n",
      "  -1.28809875e-02 -7.69478715e-02 -1.89361905e-01 -1.29621700e-01\n",
      "   3.88785844e-02  5.84837393e-03 -3.69873292e-02  3.12049480e-02\n",
      "   5.62949883e-02  2.00126390e-02  2.72583482e-02  5.00537063e-02\n",
      "   4.92539236e-02  3.49608292e-02  3.37790603e-04]\n",
      " [-3.73790477e+02  1.71397216e+02 -6.71892683e+01  4.11219346e+01\n",
      "   3.41325049e+01 -7.18160125e+01 -9.02939119e+00  1.79570808e+01\n",
      "  -3.32728485e+01 -2.77297047e+00  1.18131133e+01 -1.56487551e+01\n",
      "   7.63599179e-01 -2.23416995e+00 -1.30143968e+00  1.19884150e+00\n",
      "   9.63070411e-01  2.51417237e-01  8.54749862e-01  3.99758581e-01\n",
      "  -4.74722949e-01  3.64117262e-01  4.70890446e-01 -5.21720393e-01\n",
      "   4.48423900e-02  4.08514576e-01 -2.15544137e+00 -5.86305150e-01\n",
      "   1.29061218e+00 -3.17023572e-01 -9.69658440e-02  1.06844581e+00\n",
      "   8.58109114e-02 -2.25988493e-01  4.72052470e-01  3.21606719e-02\n",
      "  -2.48506051e-01  4.14681430e-02 -1.89870553e-01]\n",
      " [-5.57092429e+02  8.91604054e+01  8.79203397e+00  2.80527356e+01\n",
      "   2.46509070e+01  2.78204597e-02  1.37300830e+01  1.27385209e+01\n",
      "   6.52547664e-01  9.15753683e+00  4.77698948e+00 -1.09904648e+00\n",
      "   7.42396422e+00 -7.60078813e-01 -3.63659600e-01  2.54128237e-01\n",
      "  -1.28390367e-01 -4.72407143e-02  4.00514416e-01  1.76239314e-01\n",
      "  -1.36567451e-02  2.28188254e-01  1.73544907e-01 -3.25996648e-02\n",
      "   2.79089592e-02  1.23668821e-02  1.43287997e-01  8.97228227e-02\n",
      "  -3.10984999e-02 -3.11777078e-03 -1.06157581e-02 -5.97319254e-02\n",
      "  -2.05258380e-02 -3.65149361e-04 -5.29477972e-02 -4.30276791e-02\n",
      "   9.55692782e-04 -1.52542515e-02 -1.09707633e-02]\n",
      " [-3.21136858e+02  2.00125731e+02 -1.11375419e+02 -8.37418396e+00\n",
      "   3.38271131e+01 -5.31187290e+01 -1.15810922e+01  1.62489879e+01\n",
      "  -1.74632993e+01  1.29369896e-01  1.90271202e-01 -1.69116060e+01\n",
      "   9.93980451e-01  2.97145558e+00  3.99093901e-01 -1.64124799e+00\n",
      "   1.20620336e+00 -2.38671590e-01 -1.60512096e+00  7.57512067e-01\n",
      "   3.90039775e-01 -5.39041724e-01  6.01328285e-01  4.60181971e-02\n",
      "  -3.40559660e-01  4.87723795e-01 -1.93064869e+00 -7.55673721e-01\n",
      "   1.37768240e+00  5.88605964e-01 -1.06633581e-01  3.94122438e-01\n",
      "   2.06657908e-01 -2.67371666e-01 -9.39215042e-02  1.06769757e-01\n",
      "   8.94603738e-02  8.14273997e-02  6.15123752e-02]\n",
      " [-5.98340683e+02  6.56874451e+01  7.83009216e+00  3.11116239e+00\n",
      "   8.70581536e+00 -3.07715267e-01 -7.03320736e-01  3.86520562e-01\n",
      "  -4.30867762e+00 -1.26662013e+00  1.93895988e+00 -1.46901429e+00\n",
      "  -3.86512944e-01  2.86378938e+00  2.02591788e+00 -6.40286926e-01\n",
      "  -5.58171820e-01  1.07356856e-01 -2.83681342e-01 -3.99096007e-01\n",
      "  -2.35513966e-01 -4.95300488e-01 -4.33381623e-01 -3.34049487e-02\n",
      "  -1.03417261e-01 -2.43501045e-01  1.53441185e+00  9.81443306e-01\n",
      "  -3.87397157e-01 -2.44271440e-01 -1.59779430e-01 -3.72512791e-01\n",
      "  -1.26803338e-01 -4.98401215e-02 -1.52360728e-01  2.76196765e-02\n",
      "   2.45935543e-02 -1.15806212e-01 -2.82974444e-02]]\n",
      "MAP adaptation for chunk-00.wav\n",
      "-1511561381.2640169\n",
      "MAP adaptation for chunk-01.wav\n",
      "-277508858.69422406\n",
      "MAP adaptation for chunk-02.wav\n",
      "-557168569.1909447\n",
      "MAP adaptation for chunk-03.wav\n",
      "-169890106.57827723\n",
      "MAP adaptation for chunk-04.wav\n",
      "-863906510.789961\n",
      "MAP adaptation for chunk-05.wav\n",
      "-378272453.8110524\n",
      "MAP adaptation for chunk-06.wav\n",
      "-64846215.37088187\n",
      "MAP adaptation for chunk-07.wav\n",
      "-513582158.0842959\n",
      "MAP adaptation for chunk-08.wav\n",
      "-772845102.4814873\n",
      "MAP adaptation for chunk-09.wav\n",
      "-332252869.63702345\n",
      "MAP adaptation for chunk-10.wav\n",
      "-206459444.02371848\n",
      "MAP adaptation for chunk-11.wav\n",
      "-410246891.98458725\n",
      "MAP adaptation for chunk-12.wav\n",
      "-671311670.860073\n",
      "MAP adaptation for chunk-13.wav\n",
      "-1601832974.4871097\n",
      "MAP adaptation for chunk-14.wav\n",
      "-619001313.1306882\n",
      "MAP adaptation for chunk-15.wav\n",
      "-478662254.77046925\n",
      "MAP adaptation for chunk-16.wav\n",
      "-1141664946.4784253\n",
      "MAP adaptation for chunk-17.wav\n",
      "-429959818.9986589\n",
      "MAP adaptation for chunk-18.wav\n",
      "-403479608.01859623\n",
      "MAP adaptation for chunk-19.wav\n",
      "-673693794.3293387\n",
      "MAP adaptation for chunk-20.wav\n",
      "-244345157.09643304\n",
      "MAP adaptation for chunk-21.wav\n",
      "-685927391.7033949\n",
      "MAP adaptation for chunk-22.wav\n",
      "-1546875894.9389274\n",
      "MAP adaptation for chunk-23.wav\n",
      "-373195418.7453251\n",
      "[0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 3, 5, 9, 11, 14, 15, 18, 23]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################### Global GMM i.e UBM ###########################\n",
    "\n",
    "test_file_path = 'D:/Audio Analysis Notebook/Recordings/20211201-083202_8008512300_1003_TOLLFREE-all.wav'\n",
    "y,sr = librosa.load(test_file_path)\n",
    "print(np.shape(y))\n",
    "\n",
    "mfcc = librosa.feature.mfcc(np.array(y),sr,hop_length=int(hop_length * sr),n_fft=int(n_fft*sr),n_mfcc=n_mfcc,dct_type=2)\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "mfcc_delta_second_order = librosa.feature.delta(mfcc,order=2)\n",
    "temp = librosa.feature.delta(mfcc_delta)\n",
    "inter = np.vstack((mfcc,mfcc_delta,mfcc_delta_second_order))\n",
    "ubm_feature = inter.T\n",
    "#ubm_feature = preprocessing.scale(ubm_feature)\n",
    "\n",
    "# ubm_feature -= np.mean(ubm_feature)\n",
    "# ubm_feature /= np.std(ubm_feature)\n",
    "\n",
    "ubm_model = GaussianMixture(n_components = components, covariance_type = cov_type)\n",
    "ubm_model.fit(ubm_feature)\n",
    "\n",
    "print(ubm_model.score(ubm_feature))\n",
    "print(ubm_model.means_)\n",
    "\n",
    "\n",
    "def MAP_Estimation(model,data,m_iterations):\n",
    "\n",
    "    N = data.shape[0]\n",
    "    D = data.shape[1]\n",
    "    K = model.n_components\n",
    "\n",
    "\n",
    "    mu_new = np.zeros((K,D))\n",
    "    n_k = np.zeros((K,1))\n",
    "\n",
    "    mu_k = model.means_\n",
    "    \n",
    "    pi_k = model.weights_\n",
    "\n",
    "    old_likelihood = model.score(data)\n",
    "    new_likelihood = 0\n",
    "    iterations = 0\n",
    "    while(iterations < m_iterations):\n",
    "        iterations += 1\n",
    "        old_likelihood = new_likelihood\n",
    "        z_n_k = model.predict_proba(data)\n",
    "        n_k = np.sum(z_n_k,axis = 0)\n",
    "        n_k = n_k.reshape(np.shape(n_k)[0],1)\n",
    "\n",
    "        mu_new = np.dot(z_n_k.T,data)\n",
    "        n_k[n_k == 0] = 1e-20\n",
    "        mu_new = mu_new / n_k\n",
    "\n",
    "        adaptation_coefficient = n_k/(n_k + relevance_factor)\n",
    "        I = np.ones(shape=np.shape(n_k))\n",
    "        # for k in range(K):\n",
    "        #     mu_k[k] = (adaptation_coefficient[k] * mu_new[k]) + ((1 - adaptation_coefficient[k]) * mu_k[k])\n",
    "        mu_k = (adaptation_coefficient*mu_new) + (( I - adaptation_coefficient) * mu_k)\n",
    "        model.means_ = mu_k\n",
    "\n",
    "        log_likelihood = model.score(data)\n",
    "\n",
    "        new_likelihood = log_likelihood\n",
    "\n",
    "        if abs(old_likelihood - new_likelihood) < 1e-20:\n",
    "            break\n",
    "        print(log_likelihood)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "Total = []\n",
    "relevance_factor = 16\n",
    "for i in range(c):\n",
    "    fname='chunk-%002d.wav' % (i,)\n",
    "    print('MAP adaptation for {0}'.format(fname))\n",
    "    temp_y,sr_temp = librosa.load(fname,sr=None)\n",
    "    \n",
    "    temp_mfcc = librosa.feature.mfcc(np.array(temp_y),sr_temp,hop_length=int(hop_length * sr_temp),n_fft=int(n_fft*sr_temp),n_mfcc=n_mfcc,dct_type=2)\n",
    "    temp_mfcc_delta = librosa.feature.delta(temp_mfcc)\n",
    "    temp_mfcc_delta_second_order = librosa.feature.delta(temp_mfcc,order=2)\n",
    "    temp_inter = np.vstack((temp_mfcc,temp_mfcc_delta,temp_mfcc_delta_second_order))\n",
    "    temp_gmm_feature = temp_inter.T\n",
    "    #data = preprocessing.scale(temp_gmm_feature)\n",
    "\n",
    "    gmm  = deepcopy(ubm_model)\n",
    "\n",
    "    gmm = MAP_Estimation(gmm,temp_gmm_feature,m_iterations =1)\n",
    "    \n",
    "    sv = gmm.means_.flatten()\n",
    "    #sv = preprocessing.scale(sv)\n",
    "    Total.append(sv)\n",
    "\n",
    "N_CLUSTERS = 2\n",
    "\n",
    "def rearrange(labels, n):\n",
    "    seen = set()\n",
    "    distinct = [x for x in labels if x not in seen and not seen.add(x)]\n",
    "    correct = [i for i in range(n)]\n",
    "    dict_ = dict(zip(distinct, correct))\n",
    "    return [x if x not in dict_ else dict_[x] for x in labels]\n",
    "\n",
    "sc = SpectralClustering(n_clusters=N_CLUSTERS, affinity='cosine')\n",
    "\n",
    "#Labels help us identify between chunks of customer and call center agent\n",
    "labels = sc.fit_predict(Total)\n",
    "labels = rearrange(labels, N_CLUSTERS)\n",
    "print(labels)\n",
    "\n",
    "#Since there is no way to identify the voice of a customer just from the audio\n",
    "#we have assumed that customer is the one who speaks 2nd\n",
    "#Normally the call center agent is the first one to speak and then the customer\n",
    "#If that is not the case for a specific audio, change the condition from 'x==1' to 'x==0'\n",
    "print([i for i, x in enumerate(labels) if x == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e2c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\sagar\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31dc7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached tensorflow-2.7.0-cp39-cp39-win_amd64.whl (430.8 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Installing collected packages: keras, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 1.2.0\n",
      "    Uninstalling Keras-1.2.0:\n",
      "      Successfully uninstalled Keras-1.2.0\n",
      "Successfully installed keras-2.7.0 tensorflow-2.7.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oundfile (c:\\users\\sagar\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b49072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout, Flatten, Embedding\n",
    "import pickle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a5d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Path to the folder consisting different Emotions folders\n",
    "path_data = 'D:/Audio Analysis Notebook/new data'\n",
    "\n",
    "mslen = 22050\n",
    "\n",
    "data = []\n",
    "\n",
    "max_fs = 0\n",
    "labels = []\n",
    "\n",
    "emotions = ['neutral','calm','happy','sad','angry','fearful','disgust','surprised']\n",
    "directories = os.listdir(path_data)\n",
    "\n",
    "print(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8c42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('D:/Audio Analysis Notebook/Audio-Sentiment-Analysis-master/model/feature.pkl','rb')\n",
    "feature_all = pickle.load(f2)\n",
    "f3 = open('D:/Audio Analysis Notebook/Audio-Sentiment-Analysis-master/model/label.pkl','rb')\n",
    "labels = pickle.load(f3)\n",
    "from copy import deepcopy\n",
    "y = deepcopy(labels)\n",
    "for i in range(len(y)):\n",
    "    y[i] = int(y[i])\n",
    "\n",
    "\n",
    "n_labels = len(y)\n",
    "n_unique_labels = len(np.unique(y))\n",
    "one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "f = np.arange(n_labels)\n",
    "for i in range(len(f)):\n",
    "    one_hot_encode[f[i],y[i]-1]=1\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(feature_all,one_hot_encode,test_size = 0.3,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f76379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0899 - accuracy: 0.1458\n",
      "Epoch 2/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0927 - accuracy: 0.1409\n",
      "Epoch 3/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0920 - accuracy: 0.1409\n",
      "Epoch 4/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0858 - accuracy: 0.1399\n",
      "Epoch 5/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0935 - accuracy: 0.1369\n",
      "Epoch 6/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0787 - accuracy: 0.1587\n",
      "Epoch 7/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0818 - accuracy: 0.1448\n",
      "Epoch 8/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0809 - accuracy: 0.1419\n",
      "Epoch 9/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0810 - accuracy: 0.1488\n",
      "Epoch 10/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0828 - accuracy: 0.1478\n",
      "Epoch 11/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0776 - accuracy: 0.1657\n",
      "Epoch 12/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0797 - accuracy: 0.1369\n",
      "Epoch 13/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0750 - accuracy: 0.1419\n",
      "Epoch 14/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0742 - accuracy: 0.1528\n",
      "Epoch 15/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0735 - accuracy: 0.1637\n",
      "Epoch 16/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0689 - accuracy: 0.1577\n",
      "Epoch 17/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0753 - accuracy: 0.1419\n",
      "Epoch 18/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0727 - accuracy: 0.1438\n",
      "Epoch 19/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0727 - accuracy: 0.1508\n",
      "Epoch 20/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0732 - accuracy: 0.1458\n",
      "Epoch 21/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0694 - accuracy: 0.1379\n",
      "Epoch 22/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0712 - accuracy: 0.1409\n",
      "Epoch 23/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0683 - accuracy: 0.1567\n",
      "Epoch 24/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0688 - accuracy: 0.1577\n",
      "Epoch 25/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0722 - accuracy: 0.1419\n",
      "Epoch 26/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0647 - accuracy: 0.1448\n",
      "Epoch 27/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0686 - accuracy: 0.1369\n",
      "Epoch 28/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0692 - accuracy: 0.1300\n",
      "Epoch 29/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0619 - accuracy: 0.1528\n",
      "Epoch 30/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0605 - accuracy: 0.1558\n",
      "Epoch 31/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0685 - accuracy: 0.1478\n",
      "Epoch 32/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0647 - accuracy: 0.1399\n",
      "Epoch 33/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0633 - accuracy: 0.1528\n",
      "Epoch 34/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0648 - accuracy: 0.1567\n",
      "Epoch 35/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0629 - accuracy: 0.1498\n",
      "Epoch 36/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0625 - accuracy: 0.1458\n",
      "Epoch 37/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0586 - accuracy: 0.1528\n",
      "Epoch 38/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0604 - accuracy: 0.1488\n",
      "Epoch 39/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0655 - accuracy: 0.1448\n",
      "Epoch 40/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0629 - accuracy: 0.1508\n",
      "Epoch 41/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0623 - accuracy: 0.1399\n",
      "Epoch 42/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0611 - accuracy: 0.1558\n",
      "Epoch 43/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0651 - accuracy: 0.1458\n",
      "Epoch 44/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0571 - accuracy: 0.1577\n",
      "Epoch 45/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0597 - accuracy: 0.1478\n",
      "Epoch 46/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0615 - accuracy: 0.1518\n",
      "Epoch 47/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0637 - accuracy: 0.1538\n",
      "Epoch 48/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0625 - accuracy: 0.1329\n",
      "Epoch 49/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0639 - accuracy: 0.1498\n",
      "Epoch 50/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0611 - accuracy: 0.1687\n",
      "Epoch 51/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0580 - accuracy: 0.1409\n",
      "Epoch 52/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0649 - accuracy: 0.1399\n",
      "Epoch 53/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0635 - accuracy: 0.1419\n",
      "Epoch 54/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0601 - accuracy: 0.1498\n",
      "Epoch 55/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0607 - accuracy: 0.1478\n",
      "Epoch 56/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0638 - accuracy: 0.1458\n",
      "Epoch 57/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0608 - accuracy: 0.1478\n",
      "Epoch 58/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0618 - accuracy: 0.1389\n",
      "Epoch 59/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0619 - accuracy: 0.1478\n",
      "Epoch 60/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0600 - accuracy: 0.1448\n",
      "Epoch 61/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0614 - accuracy: 0.1508\n",
      "Epoch 62/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0559 - accuracy: 0.1667\n",
      "Epoch 63/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0581 - accuracy: 0.1399\n",
      "Epoch 64/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0572 - accuracy: 0.1458\n",
      "Epoch 65/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0550 - accuracy: 0.1518\n",
      "Epoch 66/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0570 - accuracy: 0.1349\n",
      "Epoch 67/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0535 - accuracy: 0.1468\n",
      "Epoch 68/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0581 - accuracy: 0.1448\n",
      "Epoch 69/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0587 - accuracy: 0.1359\n",
      "Epoch 70/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0575 - accuracy: 0.1419\n",
      "Epoch 71/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0590 - accuracy: 0.1538\n",
      "Epoch 72/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0552 - accuracy: 0.1419\n",
      "Epoch 73/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0562 - accuracy: 0.1567\n",
      "Epoch 74/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0585 - accuracy: 0.1419\n",
      "Epoch 75/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0577 - accuracy: 0.1548\n",
      "Epoch 76/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0509 - accuracy: 0.1488\n",
      "Epoch 77/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0580 - accuracy: 0.1538\n",
      "Epoch 78/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0582 - accuracy: 0.1587\n",
      "Epoch 79/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0545 - accuracy: 0.1528\n",
      "Epoch 80/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0564 - accuracy: 0.1538\n",
      "Epoch 81/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0567 - accuracy: 0.1429\n",
      "Epoch 82/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0581 - accuracy: 0.1389\n",
      "Epoch 83/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0565 - accuracy: 0.1468\n",
      "Epoch 84/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0537 - accuracy: 0.1379\n",
      "Epoch 85/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0577 - accuracy: 0.1419\n",
      "Epoch 86/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0556 - accuracy: 0.1508\n",
      "Epoch 87/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0563 - accuracy: 0.1518\n",
      "Epoch 88/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0557 - accuracy: 0.1399\n",
      "Epoch 89/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0553 - accuracy: 0.1498\n",
      "Epoch 90/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0553 - accuracy: 0.1538\n",
      "Epoch 91/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0569 - accuracy: 0.1478\n",
      "Epoch 92/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0530 - accuracy: 0.1528\n",
      "Epoch 93/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0528 - accuracy: 0.1508\n",
      "Epoch 94/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0573 - accuracy: 0.1597\n",
      "Epoch 95/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0545 - accuracy: 0.1448\n",
      "Epoch 96/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0481 - accuracy: 0.1508\n",
      "Epoch 97/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0501 - accuracy: 0.1528\n",
      "Epoch 98/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0480 - accuracy: 0.1577\n",
      "Epoch 99/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0510 - accuracy: 0.1518\n",
      "Epoch 100/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0506 - accuracy: 0.1567\n",
      "Epoch 101/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0575 - accuracy: 0.1518\n",
      "Epoch 102/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0502 - accuracy: 0.1726\n",
      "Epoch 103/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0500 - accuracy: 0.1607\n",
      "Epoch 104/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0550 - accuracy: 0.1518\n",
      "Epoch 105/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0542 - accuracy: 0.1429\n",
      "Epoch 106/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0526 - accuracy: 0.1498\n",
      "Epoch 107/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0505 - accuracy: 0.1687\n",
      "Epoch 108/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0509 - accuracy: 0.1617\n",
      "Epoch 109/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0507 - accuracy: 0.1657\n",
      "Epoch 110/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0569 - accuracy: 0.1558\n",
      "Epoch 111/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0512 - accuracy: 0.1657\n",
      "Epoch 112/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0477 - accuracy: 0.1677\n",
      "Epoch 113/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0510 - accuracy: 0.1548\n",
      "Epoch 114/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0501 - accuracy: 0.1617\n",
      "Epoch 115/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0497 - accuracy: 0.1567\n",
      "Epoch 116/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0520 - accuracy: 0.1468\n",
      "Epoch 117/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0463 - accuracy: 0.1667\n",
      "Epoch 118/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0478 - accuracy: 0.1597\n",
      "Epoch 119/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0537 - accuracy: 0.1597\n",
      "Epoch 120/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0450 - accuracy: 0.1696\n",
      "Epoch 121/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0477 - accuracy: 0.1617\n",
      "Epoch 122/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0513 - accuracy: 0.1567\n",
      "Epoch 123/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0509 - accuracy: 0.1627\n",
      "Epoch 124/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0465 - accuracy: 0.1617\n",
      "Epoch 125/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0469 - accuracy: 0.1637\n",
      "Epoch 126/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0431 - accuracy: 0.1687\n",
      "Epoch 127/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0457 - accuracy: 0.1716\n",
      "Epoch 128/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0433 - accuracy: 0.1677\n",
      "Epoch 129/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0468 - accuracy: 0.1587\n",
      "Epoch 130/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0461 - accuracy: 0.1587\n",
      "Epoch 131/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0472 - accuracy: 0.1677\n",
      "Epoch 132/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0377 - accuracy: 0.1885\n",
      "Epoch 133/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0466 - accuracy: 0.1677\n",
      "Epoch 134/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0442 - accuracy: 0.1667\n",
      "Epoch 135/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0501 - accuracy: 0.1736\n",
      "Epoch 136/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0389 - accuracy: 0.1825\n",
      "Epoch 137/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0451 - accuracy: 0.1766\n",
      "Epoch 138/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0416 - accuracy: 0.1835\n",
      "Epoch 139/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0430 - accuracy: 0.1736\n",
      "Epoch 140/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0413 - accuracy: 0.1845\n",
      "Epoch 141/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0413 - accuracy: 0.1548\n",
      "Epoch 142/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0424 - accuracy: 0.1736\n",
      "Epoch 143/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0432 - accuracy: 0.1865\n",
      "Epoch 144/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0433 - accuracy: 0.1796\n",
      "Epoch 145/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0395 - accuracy: 0.1994\n",
      "Epoch 146/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0378 - accuracy: 0.1835\n",
      "Epoch 147/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0414 - accuracy: 0.1667\n",
      "Epoch 148/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0395 - accuracy: 0.1875\n",
      "Epoch 149/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0363 - accuracy: 0.1776\n",
      "Epoch 150/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0402 - accuracy: 0.1706\n",
      "Epoch 151/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0375 - accuracy: 0.1627\n",
      "Epoch 152/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0401 - accuracy: 0.1756\n",
      "Epoch 153/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0398 - accuracy: 0.1766\n",
      "Epoch 154/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0359 - accuracy: 0.1845\n",
      "Epoch 155/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0349 - accuracy: 0.1915\n",
      "Epoch 156/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0364 - accuracy: 0.1925\n",
      "Epoch 157/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0372 - accuracy: 0.1726\n",
      "Epoch 158/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0315 - accuracy: 0.2044\n",
      "Epoch 159/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0364 - accuracy: 0.1895\n",
      "Epoch 160/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0390 - accuracy: 0.1786\n",
      "Epoch 161/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0328 - accuracy: 0.1925\n",
      "Epoch 162/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0338 - accuracy: 0.1825\n",
      "Epoch 163/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0318 - accuracy: 0.1865\n",
      "Epoch 164/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0339 - accuracy: 0.1944\n",
      "Epoch 165/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0356 - accuracy: 0.1796\n",
      "Epoch 166/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0327 - accuracy: 0.1815\n",
      "Epoch 167/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0331 - accuracy: 0.1984\n",
      "Epoch 168/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0375 - accuracy: 0.1954\n",
      "Epoch 169/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0310 - accuracy: 0.1935\n",
      "Epoch 170/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0264 - accuracy: 0.2083\n",
      "Epoch 171/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0367 - accuracy: 0.1746\n",
      "Epoch 172/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0320 - accuracy: 0.1865\n",
      "Epoch 173/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0280 - accuracy: 0.1905\n",
      "Epoch 174/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0296 - accuracy: 0.1944\n",
      "Epoch 175/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0213 - accuracy: 0.2143\n",
      "Epoch 176/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0271 - accuracy: 0.2063\n",
      "Epoch 177/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0297 - accuracy: 0.1954\n",
      "Epoch 178/200\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 2.0281 - accuracy: 0.1865\n",
      "Epoch 179/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0293 - accuracy: 0.2113\n",
      "Epoch 180/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0269 - accuracy: 0.2014\n",
      "Epoch 181/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0261 - accuracy: 0.2024\n",
      "Epoch 182/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0204 - accuracy: 0.2054\n",
      "Epoch 183/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0277 - accuracy: 0.1944\n",
      "Epoch 184/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0225 - accuracy: 0.2054\n",
      "Epoch 185/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0270 - accuracy: 0.2083\n",
      "Epoch 186/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0220 - accuracy: 0.1935\n",
      "Epoch 187/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0189 - accuracy: 0.2073\n",
      "Epoch 188/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0245 - accuracy: 0.2083\n",
      "Epoch 189/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0249 - accuracy: 0.1944\n",
      "Epoch 190/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0247 - accuracy: 0.2183\n",
      "Epoch 191/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0261 - accuracy: 0.2093\n",
      "Epoch 192/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0190 - accuracy: 0.2044\n",
      "Epoch 193/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0221 - accuracy: 0.2044\n",
      "Epoch 194/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0197 - accuracy: 0.2143\n",
      "Epoch 195/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0146 - accuracy: 0.2063\n",
      "Epoch 196/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0200 - accuracy: 0.2123\n",
      "Epoch 197/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0203 - accuracy: 0.1915\n",
      "Epoch 198/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0163 - accuracy: 0.1994\n",
      "Epoch 199/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0156 - accuracy: 0.2014\n",
      "Epoch 200/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0154 - accuracy: 0.2202\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 2.0269 - accuracy: 0.2269\n",
      "Accuracy for model 1 : 22.685185185185187\n"
     ]
    }
   ],
   "source": [
    "########################### MODEL 1 ###########################\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1],input_dim =X_train.shape[1],kernel_initializer = 'random_uniform',activation ='relu'))\n",
    "\n",
    "model.add(Dense(400,kernel_initializer = 'random_uniform',activation ='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(200,kernel_initializer = 'random_uniform',activation ='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(100,kernel_initializer = 'random_uniform',activation ='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(y_train.shape[1],kernel_initializer = 'random_uniform',activation ='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=200,batch_size = 5,verbose=1)\n",
    "\n",
    "\n",
    "model.evaluate(X_test,y_test)\n",
    "\n",
    "mlp_model = model.to_json()\n",
    "with open('mlp_model_relu_adadelta.json','w') as j:\n",
    "    j.write(mlp_model)\n",
    "model.save_weights(\"mlp_relu_adadelta_model.h5\")\n",
    "\n",
    "y_pred_model1 = model.predict(X_test)\n",
    "y2 = np.argmax(y_pred_model1,axis=1)\n",
    "y_test2 = np.argmax(y_test , axis = 1)\n",
    "\n",
    "count = 0\n",
    "for i in range(y2.shape[0]):\n",
    "    if y2[i] == y_test2[i]:\n",
    "        count+=1\n",
    "\n",
    "print('Accuracy for model 1 : ' + str((count / y2.shape[0]) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad0a98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0951 - accuracy: 0.1270\n",
      "Epoch 2/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0850 - accuracy: 0.1319\n",
      "Epoch 3/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0906 - accuracy: 0.1240\n",
      "Epoch 4/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0871 - accuracy: 0.1369\n",
      "Epoch 5/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0874 - accuracy: 0.1280\n",
      "Epoch 6/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0841 - accuracy: 0.1270\n",
      "Epoch 7/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0832 - accuracy: 0.1230\n",
      "Epoch 8/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0811 - accuracy: 0.1300\n",
      "Epoch 9/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0838 - accuracy: 0.1349\n",
      "Epoch 10/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0817 - accuracy: 0.1339\n",
      "Epoch 11/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0820 - accuracy: 0.1448\n",
      "Epoch 12/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0834 - accuracy: 0.1280\n",
      "Epoch 13/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0787 - accuracy: 0.1399\n",
      "Epoch 14/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0788 - accuracy: 0.1310\n",
      "Epoch 15/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0769 - accuracy: 0.1528\n",
      "Epoch 16/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0783 - accuracy: 0.1448\n",
      "Epoch 17/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0734 - accuracy: 0.1577\n",
      "Epoch 18/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0759 - accuracy: 0.1329\n",
      "Epoch 19/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0724 - accuracy: 0.1329\n",
      "Epoch 20/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0734 - accuracy: 0.1290\n",
      "Epoch 21/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0742 - accuracy: 0.1369\n",
      "Epoch 22/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0749 - accuracy: 0.1151\n",
      "Epoch 23/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0736 - accuracy: 0.1369\n",
      "Epoch 24/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0753 - accuracy: 0.1438\n",
      "Epoch 25/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0713 - accuracy: 0.1478\n",
      "Epoch 26/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0727 - accuracy: 0.1329\n",
      "Epoch 27/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0709 - accuracy: 0.1379\n",
      "Epoch 28/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0693 - accuracy: 0.1429\n",
      "Epoch 29/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0721 - accuracy: 0.1379\n",
      "Epoch 30/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0713 - accuracy: 0.1389\n",
      "Epoch 31/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0727 - accuracy: 0.1379\n",
      "Epoch 32/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0702 - accuracy: 0.1300\n",
      "Epoch 33/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0674 - accuracy: 0.1577\n",
      "Epoch 34/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0690 - accuracy: 0.1458\n",
      "Epoch 35/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0676 - accuracy: 0.1339\n",
      "Epoch 36/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0688 - accuracy: 0.1438\n",
      "Epoch 37/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0665 - accuracy: 0.1339\n",
      "Epoch 38/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0615 - accuracy: 0.1597\n",
      "Epoch 39/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0663 - accuracy: 0.1399\n",
      "Epoch 40/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0674 - accuracy: 0.1577\n",
      "Epoch 41/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0687 - accuracy: 0.1300\n",
      "Epoch 42/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0668 - accuracy: 0.1329\n",
      "Epoch 43/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0680 - accuracy: 0.1280\n",
      "Epoch 44/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0645 - accuracy: 0.1429\n",
      "Epoch 45/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0632 - accuracy: 0.1567\n",
      "Epoch 46/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0590 - accuracy: 0.1567\n",
      "Epoch 47/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0666 - accuracy: 0.1409\n",
      "Epoch 48/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0604 - accuracy: 0.1419\n",
      "Epoch 49/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0602 - accuracy: 0.1498\n",
      "Epoch 50/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0622 - accuracy: 0.1429\n",
      "Epoch 51/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0637 - accuracy: 0.1458\n",
      "Epoch 52/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0612 - accuracy: 0.1448\n",
      "Epoch 53/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0594 - accuracy: 0.1379\n",
      "Epoch 54/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0623 - accuracy: 0.1399\n",
      "Epoch 55/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0614 - accuracy: 0.1538\n",
      "Epoch 56/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0640 - accuracy: 0.1339\n",
      "Epoch 57/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0575 - accuracy: 0.1567\n",
      "Epoch 58/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0588 - accuracy: 0.1548\n",
      "Epoch 59/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0647 - accuracy: 0.1329\n",
      "Epoch 60/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0617 - accuracy: 0.1538\n",
      "Epoch 61/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0606 - accuracy: 0.1667\n",
      "Epoch 62/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0629 - accuracy: 0.1260\n",
      "Epoch 63/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0598 - accuracy: 0.1339\n",
      "Epoch 64/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0600 - accuracy: 0.1607\n",
      "Epoch 65/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0611 - accuracy: 0.1468\n",
      "Epoch 66/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0566 - accuracy: 0.1399\n",
      "Epoch 67/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0579 - accuracy: 0.1468\n",
      "Epoch 68/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0607 - accuracy: 0.1468\n",
      "Epoch 69/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0559 - accuracy: 0.1567\n",
      "Epoch 70/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0582 - accuracy: 0.1548\n",
      "Epoch 71/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0600 - accuracy: 0.1468\n",
      "Epoch 72/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0607 - accuracy: 0.1448\n",
      "Epoch 73/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0603 - accuracy: 0.1488\n",
      "Epoch 74/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0576 - accuracy: 0.1558\n",
      "Epoch 75/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0554 - accuracy: 0.1558\n",
      "Epoch 76/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0546 - accuracy: 0.1429\n",
      "Epoch 77/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0591 - accuracy: 0.1518\n",
      "Epoch 78/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0562 - accuracy: 0.1776\n",
      "Epoch 79/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0560 - accuracy: 0.1379: 0s - loss: 2.0547 - accuracy: \n",
      "Epoch 80/200\n",
      "202/202 [==============================] - 1s 2ms/step - loss: 2.0569 - accuracy: 0.1548\n",
      "Epoch 81/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0536 - accuracy: 0.1766\n",
      "Epoch 82/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0537 - accuracy: 0.1617\n",
      "Epoch 83/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0534 - accuracy: 0.1756\n",
      "Epoch 84/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0562 - accuracy: 0.1726\n",
      "Epoch 85/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0537 - accuracy: 0.1657\n",
      "Epoch 86/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0564 - accuracy: 0.1597\n",
      "Epoch 87/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0569 - accuracy: 0.1518\n",
      "Epoch 88/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0557 - accuracy: 0.1548\n",
      "Epoch 89/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0533 - accuracy: 0.1647\n",
      "Epoch 90/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0561 - accuracy: 0.1508\n",
      "Epoch 91/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0495 - accuracy: 0.1627\n",
      "Epoch 92/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0500 - accuracy: 0.1796\n",
      "Epoch 93/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0524 - accuracy: 0.1796\n",
      "Epoch 94/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0589 - accuracy: 0.1339\n",
      "Epoch 95/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0578 - accuracy: 0.1567\n",
      "Epoch 96/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0545 - accuracy: 0.1538\n",
      "Epoch 97/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0514 - accuracy: 0.1716\n",
      "Epoch 98/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0524 - accuracy: 0.1438\n",
      "Epoch 99/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0527 - accuracy: 0.1597\n",
      "Epoch 100/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0503 - accuracy: 0.1746\n",
      "Epoch 101/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0522 - accuracy: 0.1677\n",
      "Epoch 102/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0531 - accuracy: 0.1498\n",
      "Epoch 103/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0505 - accuracy: 0.1647\n",
      "Epoch 104/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0547 - accuracy: 0.1498\n",
      "Epoch 105/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0533 - accuracy: 0.1567\n",
      "Epoch 106/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0540 - accuracy: 0.1567TA: 0s - loss: 2.0593 - ac\n",
      "Epoch 107/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0529 - accuracy: 0.1667\n",
      "Epoch 108/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0536 - accuracy: 0.1567\n",
      "Epoch 109/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0518 - accuracy: 0.1706\n",
      "Epoch 110/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0491 - accuracy: 0.1776\n",
      "Epoch 111/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0501 - accuracy: 0.1667\n",
      "Epoch 112/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0524 - accuracy: 0.1438\n",
      "Epoch 113/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0516 - accuracy: 0.1627\n",
      "Epoch 114/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0526 - accuracy: 0.1677\n",
      "Epoch 115/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0571 - accuracy: 0.1409\n",
      "Epoch 116/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0489 - accuracy: 0.1696\n",
      "Epoch 117/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0529 - accuracy: 0.1736\n",
      "Epoch 118/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0523 - accuracy: 0.1677\n",
      "Epoch 119/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0530 - accuracy: 0.1627\n",
      "Epoch 120/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0495 - accuracy: 0.1587\n",
      "Epoch 121/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0517 - accuracy: 0.1558\n",
      "Epoch 122/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0534 - accuracy: 0.1607\n",
      "Epoch 123/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0540 - accuracy: 0.1508\n",
      "Epoch 124/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0501 - accuracy: 0.1667\n",
      "Epoch 125/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0505 - accuracy: 0.1518\n",
      "Epoch 126/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0548 - accuracy: 0.1468\n",
      "Epoch 127/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0503 - accuracy: 0.1587\n",
      "Epoch 128/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0519 - accuracy: 0.1696\n",
      "Epoch 129/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0495 - accuracy: 0.1468\n",
      "Epoch 130/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0501 - accuracy: 0.1786\n",
      "Epoch 131/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0482 - accuracy: 0.1796\n",
      "Epoch 132/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0561 - accuracy: 0.1310\n",
      "Epoch 133/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0460 - accuracy: 0.1667\n",
      "Epoch 134/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0489 - accuracy: 0.1597\n",
      "Epoch 135/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0504 - accuracy: 0.1716\n",
      "Epoch 136/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0505 - accuracy: 0.1657\n",
      "Epoch 137/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0463 - accuracy: 0.1667\n",
      "Epoch 138/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0480 - accuracy: 0.1944\n",
      "Epoch 139/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0458 - accuracy: 0.1935\n",
      "Epoch 140/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0483 - accuracy: 0.1627\n",
      "Epoch 141/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0415 - accuracy: 0.1905\n",
      "Epoch 142/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0464 - accuracy: 0.1746\n",
      "Epoch 143/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0463 - accuracy: 0.1667\n",
      "Epoch 144/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0504 - accuracy: 0.1617\n",
      "Epoch 145/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0507 - accuracy: 0.1736\n",
      "Epoch 146/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0490 - accuracy: 0.1706\n",
      "Epoch 147/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0484 - accuracy: 0.1746\n",
      "Epoch 148/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0444 - accuracy: 0.1815\n",
      "Epoch 149/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0445 - accuracy: 0.1845\n",
      "Epoch 150/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0413 - accuracy: 0.1994\n",
      "Epoch 151/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0451 - accuracy: 0.1776\n",
      "Epoch 152/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0459 - accuracy: 0.1657\n",
      "Epoch 153/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0445 - accuracy: 0.1855\n",
      "Epoch 154/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0463 - accuracy: 0.1786\n",
      "Epoch 155/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0450 - accuracy: 0.1806\n",
      "Epoch 156/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0462 - accuracy: 0.1706\n",
      "Epoch 157/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0452 - accuracy: 0.1766\n",
      "Epoch 158/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0432 - accuracy: 0.1746\n",
      "Epoch 159/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0429 - accuracy: 0.1935\n",
      "Epoch 160/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0440 - accuracy: 0.1865\n",
      "Epoch 161/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0458 - accuracy: 0.1756\n",
      "Epoch 162/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0373 - accuracy: 0.2044\n",
      "Epoch 163/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0455 - accuracy: 0.1885\n",
      "Epoch 164/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0451 - accuracy: 0.1706\n",
      "Epoch 165/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0421 - accuracy: 0.1776\n",
      "Epoch 166/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0399 - accuracy: 0.1905\n",
      "Epoch 167/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0415 - accuracy: 0.1845\n",
      "Epoch 168/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0429 - accuracy: 0.1964\n",
      "Epoch 169/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0428 - accuracy: 0.1766\n",
      "Epoch 170/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0417 - accuracy: 0.1925\n",
      "Epoch 171/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0425 - accuracy: 0.1925\n",
      "Epoch 172/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0369 - accuracy: 0.2014\n",
      "Epoch 173/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0418 - accuracy: 0.2034\n",
      "Epoch 174/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0420 - accuracy: 0.1687\n",
      "Epoch 175/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0373 - accuracy: 0.1974\n",
      "Epoch 176/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0403 - accuracy: 0.1984\n",
      "Epoch 177/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0396 - accuracy: 0.1994\n",
      "Epoch 178/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0377 - accuracy: 0.1974\n",
      "Epoch 179/200\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 2.0418 - accuracy: 0.1746\n",
      "Epoch 180/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0403 - accuracy: 0.1885\n",
      "Epoch 181/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0386 - accuracy: 0.1895\n",
      "Epoch 182/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0382 - accuracy: 0.2073\n",
      "Epoch 183/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0402 - accuracy: 0.1776\n",
      "Epoch 184/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0371 - accuracy: 0.1974\n",
      "Epoch 185/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0420 - accuracy: 0.1915\n",
      "Epoch 186/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0379 - accuracy: 0.2073\n",
      "Epoch 187/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0353 - accuracy: 0.1994\n",
      "Epoch 188/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0397 - accuracy: 0.1796\n",
      "Epoch 189/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0370 - accuracy: 0.2004\n",
      "Epoch 190/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0400 - accuracy: 0.1905\n",
      "Epoch 191/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0376 - accuracy: 0.1895\n",
      "Epoch 192/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0356 - accuracy: 0.2153\n",
      "Epoch 193/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0372 - accuracy: 0.1905\n",
      "Epoch 194/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0377 - accuracy: 0.2024\n",
      "Epoch 195/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0367 - accuracy: 0.1895\n",
      "Epoch 196/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0349 - accuracy: 0.2083\n",
      "Epoch 197/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0365 - accuracy: 0.1974\n",
      "Epoch 198/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0351 - accuracy: 0.1815\n",
      "Epoch 199/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0345 - accuracy: 0.1845\n",
      "Epoch 200/200\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 2.0361 - accuracy: 0.1875\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 2.0529 - accuracy: 0.1759\n",
      "Accuracy for model 2 : 17.59259259259259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################### MODEL 2 ###########################\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(X_train.shape[1],input_dim =X_train.shape[1],kernel_initializer = 'random_uniform',activation ='relu'))\n",
    "\n",
    "model2.add(Dense(400,kernel_initializer = 'random_uniform',activation ='tanh'))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(200,kernel_initializer = 'random_uniform',activation ='tanh'))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(100,kernel_initializer = 'random_uniform',activation ='sigmoid'))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(y_train.shape[1],kernel_initializer = 'random_uniform',activation ='softmax'))\n",
    "\n",
    "model2.compile(loss = 'categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train,y_train,epochs=200,batch_size = 5,verbose=1)\n",
    "\n",
    "model2.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "mlp_model2 = model2.to_json()\n",
    "with open('mlp_model_tanh_adadelta.json','w') as j:\n",
    "    j.write(mlp_model2)\n",
    "model2.save_weights(\"mlp_tanh_adadelta_model.h5\")\n",
    "\n",
    "\n",
    "y_pred_model2 = model2.predict(X_test)\n",
    "y22 = np.argmax(y_pred_model2,axis=1)\n",
    "y_test22 = np.argmax(y_test , axis = 1)\n",
    "\n",
    "count = 0\n",
    "for i in range(y22.shape[0]):\n",
    "    if y22[i] == y_test22[i]:\n",
    "        count+=1\n",
    "        \n",
    "print('Accuracy for model 2 : ' + str((count / y22.shape[0]) * 100))\n",
    "\n",
    "\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(feature_all,y,test_size = 0.3,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "510890bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.53040\n",
      "[1]\tvalidation_0-mlogloss:1.19694\n",
      "[2]\tvalidation_0-mlogloss:0.95765\n",
      "[3]\tvalidation_0-mlogloss:0.78845\n",
      "[4]\tvalidation_0-mlogloss:0.63909\n",
      "[5]\tvalidation_0-mlogloss:0.53076\n",
      "[6]\tvalidation_0-mlogloss:0.44669\n",
      "[7]\tvalidation_0-mlogloss:0.37746\n",
      "[8]\tvalidation_0-mlogloss:0.32135\n",
      "[9]\tvalidation_0-mlogloss:0.27689\n",
      "[10]\tvalidation_0-mlogloss:0.24327\n",
      "[11]\tvalidation_0-mlogloss:0.21211\n",
      "[12]\tvalidation_0-mlogloss:0.18402\n",
      "[13]\tvalidation_0-mlogloss:0.15976\n",
      "[14]\tvalidation_0-mlogloss:0.14240\n",
      "[15]\tvalidation_0-mlogloss:0.12492\n",
      "[16]\tvalidation_0-mlogloss:0.11215\n",
      "[17]\tvalidation_0-mlogloss:0.09975\n",
      "[18]\tvalidation_0-mlogloss:0.08961\n",
      "[19]\tvalidation_0-mlogloss:0.08074\n",
      "[20]\tvalidation_0-mlogloss:0.07281\n",
      "[21]\tvalidation_0-mlogloss:0.06615\n",
      "[22]\tvalidation_0-mlogloss:0.06038\n",
      "[23]\tvalidation_0-mlogloss:0.05538\n",
      "[24]\tvalidation_0-mlogloss:0.05137\n",
      "[25]\tvalidation_0-mlogloss:0.04765\n",
      "[26]\tvalidation_0-mlogloss:0.04413\n",
      "[27]\tvalidation_0-mlogloss:0.04117\n",
      "[28]\tvalidation_0-mlogloss:0.03864\n",
      "[29]\tvalidation_0-mlogloss:0.03641\n",
      "[30]\tvalidation_0-mlogloss:0.03442\n",
      "[31]\tvalidation_0-mlogloss:0.03260\n",
      "[32]\tvalidation_0-mlogloss:0.03098\n",
      "[33]\tvalidation_0-mlogloss:0.02960\n",
      "[34]\tvalidation_0-mlogloss:0.02825\n",
      "[35]\tvalidation_0-mlogloss:0.02702\n",
      "[36]\tvalidation_0-mlogloss:0.02590\n",
      "[37]\tvalidation_0-mlogloss:0.02488\n",
      "[38]\tvalidation_0-mlogloss:0.02395\n",
      "[39]\tvalidation_0-mlogloss:0.02306\n",
      "[40]\tvalidation_0-mlogloss:0.02223\n",
      "[41]\tvalidation_0-mlogloss:0.02152\n",
      "[42]\tvalidation_0-mlogloss:0.02084\n",
      "[43]\tvalidation_0-mlogloss:0.02020\n",
      "[44]\tvalidation_0-mlogloss:0.01960\n",
      "[45]\tvalidation_0-mlogloss:0.01902\n",
      "[46]\tvalidation_0-mlogloss:0.01855\n",
      "[47]\tvalidation_0-mlogloss:0.01802\n",
      "[48]\tvalidation_0-mlogloss:0.01756\n",
      "[49]\tvalidation_0-mlogloss:0.01712\n",
      "[50]\tvalidation_0-mlogloss:0.01672\n",
      "[51]\tvalidation_0-mlogloss:0.01635\n",
      "[52]\tvalidation_0-mlogloss:0.01598\n",
      "[53]\tvalidation_0-mlogloss:0.01566\n",
      "[54]\tvalidation_0-mlogloss:0.01533\n",
      "[55]\tvalidation_0-mlogloss:0.01501\n",
      "[56]\tvalidation_0-mlogloss:0.01473\n",
      "[57]\tvalidation_0-mlogloss:0.01444\n",
      "[58]\tvalidation_0-mlogloss:0.01418\n",
      "[59]\tvalidation_0-mlogloss:0.01393\n",
      "[60]\tvalidation_0-mlogloss:0.01367\n",
      "[61]\tvalidation_0-mlogloss:0.01344\n",
      "[62]\tvalidation_0-mlogloss:0.01323\n",
      "[63]\tvalidation_0-mlogloss:0.01300\n",
      "[64]\tvalidation_0-mlogloss:0.01281\n",
      "[65]\tvalidation_0-mlogloss:0.01262\n",
      "[66]\tvalidation_0-mlogloss:0.01243\n",
      "[67]\tvalidation_0-mlogloss:0.01227\n",
      "[68]\tvalidation_0-mlogloss:0.01209\n",
      "[69]\tvalidation_0-mlogloss:0.01193\n",
      "[70]\tvalidation_0-mlogloss:0.01178\n",
      "[71]\tvalidation_0-mlogloss:0.01162\n",
      "[72]\tvalidation_0-mlogloss:0.01148\n",
      "[73]\tvalidation_0-mlogloss:0.01133\n",
      "[74]\tvalidation_0-mlogloss:0.01119\n",
      "[75]\tvalidation_0-mlogloss:0.01104\n",
      "[76]\tvalidation_0-mlogloss:0.01093\n",
      "[77]\tvalidation_0-mlogloss:0.01081\n",
      "[78]\tvalidation_0-mlogloss:0.01069\n",
      "[79]\tvalidation_0-mlogloss:0.01059\n",
      "[80]\tvalidation_0-mlogloss:0.01049\n",
      "[81]\tvalidation_0-mlogloss:0.01039\n",
      "[82]\tvalidation_0-mlogloss:0.01027\n",
      "[83]\tvalidation_0-mlogloss:0.01017\n",
      "[84]\tvalidation_0-mlogloss:0.01007\n",
      "[85]\tvalidation_0-mlogloss:0.00997\n",
      "[86]\tvalidation_0-mlogloss:0.00989\n",
      "[87]\tvalidation_0-mlogloss:0.00980\n",
      "[88]\tvalidation_0-mlogloss:0.00971\n",
      "[89]\tvalidation_0-mlogloss:0.00963\n",
      "[90]\tvalidation_0-mlogloss:0.00955\n",
      "[91]\tvalidation_0-mlogloss:0.00947\n",
      "[92]\tvalidation_0-mlogloss:0.00939\n",
      "[93]\tvalidation_0-mlogloss:0.00931\n",
      "[94]\tvalidation_0-mlogloss:0.00924\n",
      "[95]\tvalidation_0-mlogloss:0.00918\n",
      "[96]\tvalidation_0-mlogloss:0.00910\n",
      "[97]\tvalidation_0-mlogloss:0.00903\n",
      "[98]\tvalidation_0-mlogloss:0.00897\n",
      "[99]\tvalidation_0-mlogloss:0.00890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:42:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy for model 3 : 56.25\n"
     ]
    }
   ],
   "source": [
    "########################### MODEL 3 ###########################\n",
    "model3 = XGBClassifier()\n",
    "\n",
    "evals_result = {}\n",
    "eval_s = [(X_train2, y_train2)]\n",
    "\n",
    "model3.fit(X_train2,y_train2,eval_set=eval_s)\n",
    "model3.evals_result()\n",
    "score = cross_val_score(model3, X_train2, y_train2, cv=5)\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "count = 0\n",
    "for i in range(y_pred3.shape[0]):\n",
    "    if y_pred3[i] == y_test2[i]:\n",
    "        count+=1   \n",
    "        \n",
    "print('Accuracy for model 3 : ' + str((count / y_pred3.shape[0]) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ccd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
